{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkSm0luNUmj0"
      },
      "source": [
        "# Big Data Platforms\n",
        "## Assignment 2: MapReduce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBTnDbtGUmj3"
      },
      "source": [
        "**The goal of this assignment is to:**\n",
        "- Understand and practice the details of MapReduceEngine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOj7m89PUmj4"
      },
      "source": [
        "**Prerequisites**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xYvhfO9XUmj5"
      },
      "outputs": [],
      "source": [
        "# example\n",
        "!pip install --quiet zipfile36"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmmrMQogUmj6"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ByXkrP4DUmj7"
      },
      "outputs": [],
      "source": [
        "# general\n",
        "import os\n",
        "import time\n",
        "import csv\n",
        "import random\n",
        "import warnings\n",
        "import sqlite3\n",
        "import glob\n",
        "import queue \n",
        "import concurrent.futures\n",
        "from threading import Thread\n",
        "from queue import Queue\n",
        "from time import sleep\n",
        "import pathlib\n",
        "import regex as re\n",
        "\n",
        "# ml\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "\n",
        "# visual\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# notebook\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVSntJk7Umj8"
      },
      "source": [
        "**Hide Warnings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZM712b2gUmj9"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHyOPTeVUmj9"
      },
      "source": [
        "**Disable Autoscrolling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "TKYhIW2QUmj9",
        "outputId": "b366b965-3680-49a2-f910-76c179ed293b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
              "    return false;\n",
              "}"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "%%javascript\n",
        "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
        "    return false;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgZqC3g_Umj-"
      },
      "source": [
        "**Set Random Seed**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LAV52lNSUmj_"
      },
      "outputs": [],
      "source": [
        "random.seed(123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5madIjfrUmj_"
      },
      "source": [
        "# Initial Steps\n",
        "\n",
        "Write Python code to create 20 different CSV files in this format:  `myCSV[Number].csv`, where each file contains 10 records. \n",
        "\n",
        "The schema is `(‘firstname’,’secondname’,city’)`  \n",
        "\n",
        "Values should be randomly chosen from the lists: \n",
        "- `firstname` : `[John, Dana, Scott, Marc, Steven, Michael, Albert, Johanna]`  \n",
        "- `city` : `[New York, Haifa, München, London, Palo Alto,  Tel Aviv, Kiel, Hamburg]`  \n",
        "- `secondname`: any value  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "39BWLX3IUmkA"
      },
      "outputs": [],
      "source": [
        "# declared lists that values should be taken from\n",
        "firstname = ['John', 'Dana', 'Scott', 'Marc', 'Steven', 'Michael', 'Albert', 'Johanna']\n",
        "secondname = ['Wang', 'Vuitton', 'Gabanna', 'Kors', 'Jacobs', 'Stileto', 'Armani', 'Boss']\n",
        "city = ['NewYork', 'Haifa', 'Munchen', 'London', 'PaloAlto',  'TelAviv', 'Kiel', 'Hamburg'] \n",
        "\n",
        "# generate 20 files\n",
        "for file_number in range(1, 21):\n",
        "  \n",
        "  # assemble file name according to format\n",
        "  file_name = \"myCSV[%s].csv\" % file_number\n",
        "\n",
        "  # open the file and write 10 lines\n",
        "  with open(file_name, 'w') as file:\n",
        "\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"firstname\", \"secondname\", \"city\"])\n",
        "\n",
        "    for row in range(0, 10):\n",
        "        row = [random.choice(firstname), random.choice(secondname), random.choice(city)]\n",
        "        writer.writerow(row)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP1tYm19UmkB"
      },
      "source": [
        "Create mapreducetemp and mapreducefinal folders in your laptop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fbMoiMtWUmkB"
      },
      "outputs": [],
      "source": [
        "# create directories\n",
        "os.mkdir('mapreducetemp') \n",
        "os.mkdir('mapreducefinal') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwZ9E3_wUmkB"
      },
      "source": [
        "# Task 1: Map Reduce Engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqLuBUZyUmkB"
      },
      "source": [
        "1 - Write Python code to create an SQLite database with the following table\n",
        "\n",
        "`TableName: temp_results`   \n",
        "`schema: (key:TEXT,value:TEXT)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Taw06KXvUmkB"
      },
      "outputs": [],
      "source": [
        "# initialize a connection object\n",
        "conn = None\n",
        "\n",
        "# open / create new database\n",
        "try:\n",
        "    conn = sqlite3.connect(\"mydb.db\")\n",
        "\n",
        "except Error as e:\n",
        "    print(e)\n",
        "\n",
        "# if it opens without errors, \n",
        "finally:\n",
        "\n",
        "    if conn:\n",
        "      # create a new table and execute the statement\n",
        "      cursor = conn.cursor()\n",
        "      create_table_statement = ''' CREATE TABLE IF NOT EXISTS temp_results(key TEXT, value TEXT)'''\n",
        "      cursor.execute(create_table_statement)\n",
        "      conn.commit()\n",
        "      conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtJO-A3ZUmkC"
      },
      "source": [
        "2 - Create a Python class `MapReduceEngine` with method `def execute(input_data, map_function, reduce_function)`, such that: `input_data`: is an array of elements, `map_function`: is a pointer to the Python function that returns a list where each entry of the form (key,value), `reduce_function`: is pointer to the Python function that returns a list where each entry of the form (key,value)\n",
        "\n",
        "**Implement** the following functionality in the `execute(...)` function:\n",
        "\n",
        "1. For each key  from the  input_data, start a new Python thread that executes map_function(key) \n",
        "<br>\n",
        "2. Each thread will store results of the map_function into mapreducetemp/part-tmp-X.csv where X is a unique number per each thread.\n",
        "<br>\n",
        "3. Keep the list of all threads and check whether they are completed.\n",
        "<br>\n",
        "4. Once all threads completed, load content of all CSV files into the temp_results table in SQLite.\n",
        "\n",
        "  Remark: The easiest way to loop over all CSV files and load them into Pandas first, then load into SQLite  \n",
        "    `data = pd.read_csv(path to csv)`  \n",
        "    `data.to_sql(‘temp_results’,sql_conn, if_exists=’append’,index=False)`\n",
        "<br>\n",
        "\n",
        "5. Write SQL statement that generates a sorted list by key of the form `(key, value)` where value is concatenation of ALL values in the value column that match specific key. For example, if table has records\n",
        "<table>\n",
        "    <tbody>\n",
        "            <tr>\n",
        "                <td style=\"text-align:center\">John</td>\n",
        "                <td style=\"text-align:center\">myCSV1.csv</td>\n",
        "            </tr>\n",
        "            <tr>\n",
        "                <td style=\"text-align:center\">Dana</td>\n",
        "                <td style=\"text-align:center\">myCSV5.csv</td>\n",
        "            </tr>\n",
        "            <tr>\n",
        "                <td style=\"text-align:center\">John</td>\n",
        "                <td style=\"text-align:center\">myCSV7.csv</td>\n",
        "            </tr>\n",
        "    </tbody>\n",
        "</table>\n",
        "\n",
        "    Then SQL statement will return `(‘John’,’myCSV1.csv, myCSV7.csv’)`  \n",
        "    Remark: use GROUP_CONCAT and also GROUP BY ORDER BY\n",
        "<br>\n",
        "6. Start a new thread for each value from the generated list in the previous step, to execute `reduce_function(key,value)` \n",
        "<br>    \n",
        "7. Each thread will store results of reduce_function into `mapreducefinal/part-X-final.csv` file  \n",
        "<br>\n",
        "8. Keep list of all threads and check whether they are completed  \n",
        "<br>\n",
        "9. Once all threads completed, print on the screen `MapReduce Completed`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hBX6KgN_qonp"
      },
      "outputs": [],
      "source": [
        "class MapReduceEngine():\n",
        "\n",
        "    '''\n",
        "    input_data: array of elements [‘myCSV1.csv’,.. ,‘myCSV20.csv’]\n",
        "    map_function: a pointer to the function that returns a list where each entry of the form (key,value)\n",
        "    reduce_function: a pointer to the function that returns a list where each entry of the form (key,value)\n",
        "    params: parameters to the map_function of the form params={key:value}\n",
        "    '''\n",
        "    @staticmethod\n",
        "    def execute(input_data, map_function, reduce_function, params):\n",
        "\n",
        "      threads_map = []\n",
        "      queue_map = Queue()\n",
        "\n",
        "      # for each key from the input_data\n",
        "      for document_name in input_data:\n",
        "\n",
        "        # initialize the thread a new Python thread that executes map_function(key)\n",
        "        thread = Thread(target=map_function, args=(document_name, params['column'], queue_map))\n",
        "\n",
        "        # keep list of all threads\n",
        "        threads_map.append(thread)\n",
        "\n",
        "        # start the thread\n",
        "        thread.start()\n",
        "\n",
        "      # join thread together to block until they finish\n",
        "      for thr in threads_map:\n",
        "        thr.join()\n",
        "\n",
        "      # store results of map_function into mapreducetemp/...\n",
        "      regex = re.compile(r'\\d+')\n",
        "\n",
        "      # iterate over every CSV that was created\n",
        "      for document_name in input_data:\n",
        "\n",
        "        # unique id per each thread\n",
        "        unique_id = regex.findall(document_name)[0]\n",
        "\n",
        "        # extract results\n",
        "        map_results = queue_map.get()\n",
        "\n",
        "        # create and open a temp file, write results in\n",
        "        file_name = \"mapreducetemp/part-tmp-%s.csv\" % unique_id\n",
        "        with open(file_name, 'w') as file:\n",
        "          writer = csv.writer(file)\n",
        "          writer.writerow([\"key\", \"value\"])\n",
        "          # write every list value as entry\n",
        "          for entry in map_results:\n",
        "            writer.writerow(entry)\n",
        "\n",
        "      # check whether they are completed\n",
        "      for thr in threads_map:\n",
        "        if not thr.isAlive():\n",
        "          continue\n",
        "        else:\n",
        "          print('Somethings wrong')\n",
        "\n",
        "      # once all threads are completed, load content of all CSV files into temp_results table\n",
        "      conn = sqlite3.connect(\"mydb.db\")\n",
        "      path = \"mapreducetemp\"\n",
        "      for filename in os.listdir(path):\n",
        "\n",
        "        data = pd.read_csv(path+'/'+filename)\n",
        "        data.to_sql('temp_results', conn, if_exists='append', index=False)\n",
        "\n",
        "      conn.commit()\n",
        "\n",
        "      # Create a SQL connection\n",
        "      cur = conn.cursor()\n",
        "\n",
        "      # generate sorted list (key, value) where value is concat of ALL values that match key\n",
        "      db_df = pd.read_sql_query(\"SELECT key, GROUP_CONCAT(value) as value FROM temp_results GROUP BY key ORDER BY key\", conn)\n",
        "      conn.close()\n",
        "\n",
        "      threads_reduce = []\n",
        "      queue_reduce = Queue()\n",
        "\n",
        "      # start a new thread for each key from generated list \n",
        "      for index, row in db_df.iterrows():\n",
        "        key = row['key']\n",
        "        value = row['value']\n",
        "\n",
        "        # start a new Python thread that executes map_function(key)\n",
        "        thread = Thread(target=reduce_function, args=(key, value, queue_reduce))\n",
        "\n",
        "        # keep list of all threads\n",
        "        threads_reduce.append(thread)\n",
        "\n",
        "        thread.start()\n",
        "\n",
        "      # join thread together to block until they finish\n",
        "      for thr in threads_reduce:\n",
        "        thr.join()\n",
        "\n",
        "      for thr in range(len(threads_reduce)):\n",
        "        reduce_results = queue_reduce.get()\n",
        "\n",
        "        file_name = \"mapreducefinal/part-%s-final.csv\" % thr\n",
        "        with open(file_name, 'w') as file:\n",
        "          writer = csv.writer(file)\n",
        "          writer.writerow([\"key\", \"value\"])\n",
        "\n",
        "          # write every list value as entry\n",
        "          #for entry in reduce_results:\n",
        "          writer.writerow(reduce_results)\n",
        "\n",
        "      # check whether they are completed\n",
        "      for thr in threads_map:\n",
        "        if not thr.isAlive():\n",
        "          continue\n",
        "        else:\n",
        "          print('Somethings wrong')\n",
        "      \n",
        "      return 'MapReduce Completed'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSVGDQlZUmkC"
      },
      "source": [
        "# Task 2: Implement the MapReduce inverted index of the JSON documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5M-wDnoUmkD"
      },
      "source": [
        "1 - Implement a function `inverted_map(document_name, column_index)` which reads the CSV document from the local disc and returns  a list that contains entries of the form (key_value, document_name) for the specific column_index provided.\n",
        "\n",
        "For example, if column_index = 1 and myCSV11.csv document has values like:\n",
        "<table>\n",
        "    <tbody>\n",
        "            <tr>\n",
        "                <td style=\"text-align:center\">firstname</td>\n",
        "                <td style=\"text-align:center\">secondname</td>\n",
        "                <td style=\"text-align:center\">city</td>\n",
        "            </tr>\n",
        "            <tr>\n",
        "                <td style=\"text-align:center\">Michael</td>\n",
        "                <td style=\"text-align:center\">Vernik</td>\n",
        "                <td style=\"text-align:center\">Tel Aviv</td>\n",
        "            </tr>\n",
        "            <tr>\n",
        "                <td style=\"text-align:center\">Johanna</td>\n",
        "                <td style=\"text-align:center\">Vernik</td>\n",
        "                <td style=\"text-align:center\">Hamburg</td>\n",
        "            </tr>\n",
        "            <tr>\n",
        "                <td style=\"text-align:center\">Steven</td>\n",
        "                <td style=\"text-align:center\">Friedman</td>\n",
        "                <td style=\"text-align:center\">Palo Alto</td>\n",
        "            </tr>\n",
        "                        <tr>\n",
        "                <td style=\"text-align:center\">...</td>\n",
        "                <td style=\"text-align:center\">...</td>\n",
        "                <td style=\"text-align:center\">...</td>\n",
        "            </tr>\n",
        "    </tbody>\n",
        "</table>\n",
        "\n",
        "Then `inverted_map(‘myCSV11.csv’, column_index=1)` function will return a list of the form:  \n",
        "\n",
        "<table>\n",
        "    <tbody>\n",
        "            <tr>\n",
        "                <td style=\"text-align:center\">key</td>\n",
        "                <td style=\"text-align:center\">value</td>\n",
        "            </tr>\n",
        "            <tr>\n",
        "                <td style=\"text-align:center\">Michael</td>\n",
        "                <td style=\"text-align:center\">/Users/gilv/Dev/DataStore/csv/myCSV11.csv</td>\n",
        "            </tr>\n",
        "            <tr>\n",
        "                <td style=\"text-align:center\">Johanna</td>\n",
        "                <td style=\"text-align:center\">/Users/gilv/Dev/DataStore/csv/myCSV11.csv</td>\n",
        "            </tr>\n",
        "            <tr>\n",
        "                <td style=\"text-align:center\">Steven</td>\n",
        "                <td style=\"text-align:center\">/Users/gilv/Dev/DataStore/csv/myCSV11.csv</td>\n",
        "            </tr>\n",
        "                        <tr>\n",
        "                <td style=\"text-align:center\">...</td>\n",
        "                <td style=\"text-align:center\">...</td>\n",
        "            </tr>\n",
        "    </tbody>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qOa8OEG7oEJQ"
      },
      "outputs": [],
      "source": [
        "# Read CSV file, return a list that contains entries: (key_value, document_name)\n",
        "# for the specific column index provided\n",
        "def inverted_map(document_name, column_index, queue_map):\n",
        "\n",
        "  # set column mapping\n",
        "  column_mapping = {1: 'firstname', 2: 'secondname', 3: 'city'}\n",
        "  selected_column = column_mapping[column_index]\n",
        "\n",
        "  # read csv file and loads into dataframe\n",
        "  csv_data = pd.read_csv(document_name)\n",
        "\n",
        "  # list that contains entries of the form (key_value, document_name)\n",
        "  results = []\n",
        "\n",
        "  # for every value in selected column, assemble results\n",
        "  for index, value in csv_data[selected_column].items():\n",
        "    entry = (value, document_name)\n",
        "    results.append(entry)\n",
        "\n",
        "  # [('John', '/content/myCSV[1].csv'), ('Albert', '/content/myCSV[1].csv'), ...]\n",
        "  queue_map.put(results)\n",
        "  return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzisbkylUmkD"
      },
      "source": [
        "2 - Write a reduce function `inverted_reduce(key, documents)`, where the field “documents” contains a list of all CSV documents per given key.   \n",
        "This list might have duplicates.   \n",
        "Reduce function will return new list without duplicates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hgu2UicloXj0"
      },
      "outputs": [],
      "source": [
        "# documents is a list of CSV documents per given key (might have duplicates)\n",
        "# Return a new list without duplicates\n",
        "def inverted_reduce(key, documents, queue_reduce):\n",
        "\n",
        "  # remove duplicates by creating a dictionary using list items as keys\n",
        "  documents = documents.split(\",\")\n",
        "  documents = list(set(documents))\n",
        "\n",
        "  queue_reduce.put((key, documents))\n",
        "  return (key, documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKpIK0WLUmkD"
      },
      "source": [
        "# Task 3: Submit your first MapReduce\n",
        "\n",
        "1 - Create Python list in *input_data: ['myCSV1.csv', ..., myCSV19.csv']*\n",
        "2 - Submit MapReduce as follows:\n",
        "\n",
        "  `mapreduce = MapReduceEngine()`<br>\n",
        "  `status = mapreduce.execute(input_data, inverted_map, inverted_index, params={'column':1})`<br>\n",
        "  `print(status)`<br>\n",
        "\n",
        "3 - 'MapReduce Completed' should be printed and **mapreducefinal** folder should contain the result files.\n",
        "\n",
        "4 - Delete all temporary data from **mapreducetemp** folder and delete SQLite datbase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_owGp2-cg_l",
        "outputId": "ce006f91-7f72-400b-976c-c3b382064c39"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/myCSV[11].csv',\n",
              " '/content/myCSV[3].csv',\n",
              " '/content/myCSV[17].csv',\n",
              " '/content/myCSV[9].csv',\n",
              " '/content/myCSV[14].csv',\n",
              " '/content/myCSV[16].csv',\n",
              " '/content/myCSV[1].csv',\n",
              " '/content/myCSV[8].csv',\n",
              " '/content/myCSV[12].csv',\n",
              " '/content/myCSV[15].csv',\n",
              " '/content/myCSV[20].csv',\n",
              " '/content/myCSV[6].csv',\n",
              " '/content/myCSV[10].csv',\n",
              " '/content/myCSV[19].csv',\n",
              " '/content/myCSV[13].csv',\n",
              " '/content/myCSV[4].csv',\n",
              " '/content/myCSV[18].csv',\n",
              " '/content/myCSV[7].csv',\n",
              " '/content/myCSV[2].csv',\n",
              " '/content/myCSV[5].csv']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Create Python list in input_data\n",
        "input_data = glob.glob(\"/content/*.csv\")\n",
        "input_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEg8GBb3dRCj",
        "outputId": "917307aa-2f32-4ebc-9ab1-ac8294b9120f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       key                                              value\n",
            "0   Albert  /content/myCSV[10].csv,/content/myCSV[10].csv,...\n",
            "1     Dana  /content/myCSV[10].csv,/content/myCSV[1].csv,/...\n",
            "2  Johanna  /content/myCSV[10].csv,/content/myCSV[12].csv,...\n",
            "3     John  /content/myCSV[10].csv,/content/myCSV[12].csv,...\n",
            "4     Marc  /content/myCSV[10].csv,/content/myCSV[10].csv,...\n",
            "5  Michael  /content/myCSV[12].csv,/content/myCSV[12].csv,...\n",
            "6    Scott  /content/myCSV[12].csv,/content/myCSV[12].csv,...\n",
            "7   Steven  /content/myCSV[10].csv,/content/myCSV[17].csv,...\n",
            "MapReduce Completed\n"
          ]
        }
      ],
      "source": [
        "mapreduce = MapReduceEngine()\n",
        "status = mapreduce.execute(input_data, inverted_map, inverted_reduce, params={'column':1})\n",
        "print(status)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sshhlYb42pPe"
      },
      "outputs": [],
      "source": [
        "# Delete all temporary data from mapreducetemp folder and delete SQLite datbase.\n",
        "if os.path.isdir('/content/mapreducetemp'):\n",
        "  path = '/content/mapreducetemp'\n",
        "  for filename in os.listdir(path):\n",
        "    os.remove(path+'/'+filename)\n",
        "  os.rmdir('mapreducetemp')\n",
        "\n",
        "if os.path.isfile('mydb.db'):\n",
        "  os.remove('mydb.db')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeIkVcx7UmkE"
      },
      "source": [
        "# Task 4:\n",
        "\n",
        "The phase where MapReduceEngine reads all temporary files generated by maps and sort them to provide each reducer a specific key is called the shuffle step. What would be the main problem of MapReduce when processing Big Data, if there is no shuffle step at all? (meaning reducers will directly read responses from the mappers)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XaY447VfHP8"
      },
      "source": [
        "### **Answer:**\n",
        "\n",
        "The shuffle and sort step moves all files with the same key to a specific reducer. This ensures that each reducer will receive all records associated with a specific key so that it can apply the reduce function to them. When processing big data, the reducers rely on the fact that they receive a key and the complete set of values for that key, to return the (key, value) pair for that key.\n",
        "This also allows reduce functions to be processed on different machines, and can thus be executed in parallel, making processing much faster.\n",
        "If there was no shuffle step, we would not be able to guarantee that all files with the same key will arrive at the same reducer (they would be distributed randomly across reducers and/or machines). If this were the case when the reduce function runs, it would not give the correct result as it relies on all matching records to be in order and passed to the same reducer.\n",
        "We would end up with incomplete (key, value) pairs that do not represent the true desired values.\n",
        "As well, the parallelism of MapReduce would not be able to operate."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "345428312_806709.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
